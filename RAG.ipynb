{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG bassato su notizie di giornali\n",
    "\n",
    "Benvenuti in questo notebook dedicato all'esplorazione di un sistema **RAG (Retrieval-Augmented Generation)** arricchito con funzionalità di filtering dei metadati.\n",
    "\n",
    "## Funzionalità Principali\n",
    "\n",
    "Il sistema consente di affinare la ricerca dei documenti attraverso i seguenti filtri di metadati: \n",
    "- **Sezione**: specifica la sezione dell'articolo di giornale (es. politica, esteri, cronache, ecc.).\n",
    "- **Data**: permette di selezionare articoli pubblicati in un determinato intervallo temporale.\n",
    "- **Autore**: consente di focalizzarsi sugli articoli scritti da un autore specifico.\n",
    "\n",
    "## Miglioramento dei Risultati\n",
    "\n",
    "Per ottenere risposte ancora più pertinenti, puoi sfruttare tecniche di **prompt engineering** modificando i prompt che si trovano nel file `rag_utils`. Qui potrai ottimizzare i prompt utilizzati dal generatore LLM per adattarli al tuo caso d'uso specifico.\n",
    "\n",
    "Inoltre, puoi regolare due parametri chiave del sistema:\n",
    "- **K**: definisce il numero di documenti recuperati dal retriever nella fase iniziale.\n",
    "- **top_n**: specifica quanti documenti, dopo la fase di reranking, vengono utilizzati come contesto per il modello generativo.\n",
    "\n",
    "Questi parametri permettono di bilanciare precisione e recall, garantendo un controllo granulare sui risultati.\n",
    "\n",
    "## Domanda di Esempio\n",
    "\n",
    "> Di cosa parlano gli articoli della sezione politica che parlano di Toti?\n",
    "\n",
    "Usando i filtri e i parametri descritti, potrai scoprire come configurare il sistema per rispondere a domande complesse come questa, migliorando continuamente la qualità delle risposte. \n",
    "\n",
    "---\n",
    "\n",
    "Buona esplorazione!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "import warnings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "import langchain_core\n",
    "import logging\n",
    "\n",
    "# Configura il livello di log\n",
    "logging.basicConfig(level=logging.ERROR, format=\"%(levelname)s: %(message)s\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from langchain.chains.query_constructor.base import (\n",
    "    StructuredQueryOutputParser,\n",
    "    get_query_constructor_prompt\n",
    ")\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_community.query_constructors.opensearch import OpenSearchTranslator\n",
    "from typing import Any, Dict, Callable, List, Optional\n",
    "from langchain.schema import Document\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.runnables import RunnableConfig, ensure_config\n",
    "import rag_utils\n",
    "import time\n",
    "\n",
    "''' ---------------- FILTER CORRECTION -------------------\n",
    "Funzioni custom create per verificare la correttezza del filtro sui metadati creato dall'LLM:\n",
    "nel caso in cui siano presenti filtri su campi inesistenti, questi verranno rimossi dal filtro per evitare di ottenere dei\n",
    "risultati vuoti'''\n",
    "\n",
    "def correct_structured_query(strucuterd_query):\n",
    "    if isinstance(strucuterd_query, langchain_core.structured_query.StructuredQuery):\n",
    "        if strucuterd_query.filter is not None:\n",
    "            strucuterd_query.filter = correct_filter(strucuterd_query.filter)\n",
    "    return strucuterd_query\n",
    "\n",
    "\n",
    "def correct_filter(filtro):\n",
    "    if isinstance(filtro, langchain_core.structured_query.Operation):\n",
    "        for i in range(len(filtro.arguments) - 1, -1, -1):\n",
    "            if isinstance(filtro.arguments[i], langchain_core.structured_query.Operation):\n",
    "                filtro.arguments[i] = correct_filter(filtro.arguments[i])\n",
    "            else:\n",
    "                if filtro.arguments[i].attribute not in rag_utils.useful_attributes:\n",
    "                    del filtro.arguments[i]\n",
    "        if (\n",
    "                filtro.operator == langchain_core.structured_query.Operator.AND or filtro.operator == langchain_core.structured_query.Operator.OR) and len(\n",
    "                filtro.arguments) < 2:\n",
    "            if len(filtro.arguments) == 1:\n",
    "                filtro = filtro.arguments[0]\n",
    "            else:\n",
    "                filtro = None\n",
    "    elif isinstance(filtro, langchain_core.structured_query.Comparison):\n",
    "        if filtro.attribute not in rag_utils.useful_attributes:\n",
    "            filtro = None\n",
    "            print(f\"Deleted, now: {filtro}\")\n",
    "    return filtro\n",
    "\n",
    "''' ---------------- END FILTER CORRECTION ------------------- '''\n",
    "\n",
    "class OpenSearchRetrieverWrapper:\n",
    "    def __init__(self, llm_model_name:str, ollama_url:str, vectorstore:OpenSearchVectorSearch, \n",
    "                 top_n:int, search_kwargs: dict = {\"k\": 20},**query_model_kwargs:dict):\n",
    "        constructor_prompt = get_query_constructor_prompt(\n",
    "                rag_utils.document_content_description,\n",
    "                rag_utils.metadata_field_info,\n",
    "                allowed_comparators=rag_utils.allowed_comparators,\n",
    "                examples=rag_utils.examples,\n",
    "            )\n",
    "        \n",
    "        query_model = ChatOllama(\n",
    "            model=llm_model_name, \n",
    "            base_url= ollama_url,\n",
    "            temperature=query_model_kwargs.get(\"temperature\", 0), #questo parametro rende l'LLM \"meno creativo\" nella fase di creazione del filtro\n",
    "            streaming=query_model_kwargs.get(\"streaming\", True),\n",
    "            format='json', \n",
    "            seed=1   #puoi mettere il valore che vuoi, o eliminarlo\n",
    "        )\n",
    "\n",
    "        output_parser = StructuredQueryOutputParser.from_components(fix_invalid=True)\n",
    "        self.constructor_prompt = constructor_prompt\n",
    "        # la pipeline di creazione della query presenta anche la funzione custom di filter correction\n",
    "        self.query_constructor = constructor_prompt | query_model | output_parser | correct_structured_query\n",
    "\n",
    "        # custom in quando è necessario modificare leggermente la funzione _get_docs_with_query\n",
    "        retriever = CustomSelfQueryRetriever(\n",
    "            query_constructor=self.query_constructor,\n",
    "            vectorstore=vectorstore,\n",
    "            structured_query_translator=OpenSearchTranslator(),\n",
    "            search_kwargs = search_kwargs,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # reranker\n",
    "        compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\", top_n=top_n)\n",
    "        self.retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=retriever\n",
    "        )\n",
    "        \n",
    "class CustomSelfQueryRetriever(SelfQueryRetriever):    \n",
    "    def _get_docs_with_query(self, query: str, search_kwargs: Dict[str, Any]) -> List[Document]:\n",
    "        \"\"\"Get docs, adding score information.\"\"\"\n",
    "        efficient_filter = search_kwargs.get(\"filter\", None)\n",
    "        if efficient_filter is not None:\n",
    "            return self.vectorstore.similarity_search(query=query,k=search_kwargs[\"k\"], search_type=\"approximate_search\",\n",
    "                                                      vector_field=\"embeddings\",efficient_filter=efficient_filter)\n",
    "        else:\n",
    "            return self.vectorstore.similarity_search(query=query,k=search_kwargs[\"k\"], search_type=\"approximate_search\",\n",
    "                                                      vector_field=\"embeddings\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"\"        # dove è hostato Ollama\n",
    "OPEN_SEARCH_URL = \"\"        # dove è ostato OpenSearch\n",
    "# puoi usare gli embeddings che preferisci, l'importante che siano gli stessi usati in fase di memorizzazione\n",
    "EMBEDDINGS_NAME = \"intfloat/multilingual-e5-large\"   \n",
    "DEVICE_ID = 0\n",
    "INDEX_NAME = \"\"             # nome dell'indice dove sono memorizzati i dati\n",
    "TOP_N = 4       # più è grande, più aumenti il contesto dato in pasto all'LLM in fase di generazione. Fai attenzione al problema del Lost in the middle\n",
    "K = 20          # più è grande, più sarà lento, ma aumenti la recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embeddings =  HuggingFaceEmbeddings(model_name=EMBEDDINGS_NAME, model_kwargs={\"trust_remote_code\": True, \"device\": f\"cuda:{DEVICE_ID}\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docsearch = OpenSearchVectorSearch(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    opensearch_url=OPEN_SEARCH_URL,\n",
    "    use_ssl = False,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(f\"[{index}] {doc.page_content}\" for index,doc in enumerate(docs))\n",
    "\n",
    "def get_chain(retriever, llm_generator_name, user_prompt):\n",
    "    llm = ChatOllama(model=llm_generator_name, base_url=OLLAMA_BASE_URL,temperature=0, stream=True)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                ('system',rag_utils.system_prompt),\n",
    "                ('human', user_prompt)\n",
    "            ])\n",
    "    rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    rag_chain_with_source = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    ).assign(answer=rag_chain_from_docs)\n",
    "\n",
    "    return rag_chain_with_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "LLM_QUERY_CREATOR = \"qwen2.5:7b\"    # puoi usare l'LLM che preferisci, ovviamente fai attenzione che il prompt sia ancora adatto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "opensearch_retriever_wrapper = OpenSearchRetrieverWrapper(\n",
    "    vectorstore=docsearch,\n",
    "    ollama_url=OLLAMA_BASE_URL,\n",
    "    llm_model_name=LLM_QUERY_CREATOR,\n",
    "    query_constructor_llm_model_url=OLLAMA_BASE_URL,\n",
    "    temperature=0,\n",
    "    top_n=TOP_N,\n",
    "    search_kwargs={\n",
    "        \"k\": K\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Definizione manuale dei mesi in italiano\n",
    "mesi = [\n",
    "    \"Gennaio\", \"Febbraio\", \"Marzo\", \"Aprile\", \"Maggio\", \"Giugno\",\n",
    "    \"Luglio\", \"Agosto\", \"Settembre\", \"Ottobre\", \"Novembre\", \"Dicembre\"\n",
    "]\n",
    "\n",
    "# Ottieni la data odierna\n",
    "oggi = datetime.now()\n",
    "\n",
    "# Formatta la data\n",
    "giorno = oggi.day\n",
    "mese = mesi[oggi.month - 1]  # Mese in italiano\n",
    "anno = oggi.year\n",
    "\n",
    "# Componi la data\n",
    "data_formattata = f\"{giorno} {mese} {anno}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "question = \"Di cosa parlano gli articoli della sezione politica che parlano di Toti?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oggi è 18 Dicembre 2024. Di cosa parlano gli articoli della sezione politica che parlano di Toti?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Questo passaggio non è necessario, ho deciso io di impostare le domande sempre in questo modo. Puoi fare il prompt engeneering che vuoi\n",
    "# fai attenzione agli esempi in rag_utils.py\n",
    "question = f\"Oggi è {data_formattata}. {question}\"\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter creation\n",
    "Vediamo come viene creato il filtro per recuperare i documenti rilevanti da parte dell'LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredQuery(query='Di cosa parlano gli articoli che parlano di Toti?', filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='date', value={'date': '2024-12-18', 'type': 'date'}), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='section', value='politica')]), limit=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_query = opensearch_retriever_wrapper.query_constructor.invoke(question)\n",
    "structured_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "Vediamo quali sono i documenti ritrovati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '4acf1077-8f43-4d60-a480-062c7b8bad98', 'relevance_score': 0.0062182066, 'author': 'Claudio Bozza', 'date': '2024-12-18T11:46:55+0100', 'type': 'content', 'section': 'politica', 'url': 'https://www.corriere.it/politica/24_dicembre_18/valditara-fa-causa-a-lagioia-il-ministro-chiede-20-mila-euro-allo-scrittore-ecco-perche-4d6d16fa-cfc0-434b-a431-184bf2bddxlk.shtml'}, page_content=\"\\nValditara, nelle settimane scorse, aveva avviato una causa civile anche contro lo scrittore e insegnante Christian Raimo, che in tv aveva definito\\xa0«cialtrone» il ministro. L'ufficio scolastico regionale ha poi sospeso Raimo per tre mesi dalla cattedra, con decurtazione del 50% dello stipendio. Così, dopo questo nuovo procedimento, è riesplosa la bufera politica: «Chiediamo al ministro e alla premier Meloni se esista ancora in questo Paese la libertà di manifestare il proprio pensiero. Valditara - ha detto la leader del Pd, Elly Schlein - si sta infatti distinguendo per la serie di denunce e querele che rivolge a chi lo critica o esprime giudizi negativi sul suo operato. Una lista inaugurata da Christian Raimo e proseguita oggi con lo scrittore Nicola Lagioia e con il giornalista Giulio Cavalli».\\xa0\"),\n",
       " Document(metadata={'id': 'f76a3639-0694-40c7-9734-193a4d77487b', 'relevance_score': 0.003575257, 'author': 'Redazione Politica', 'date': '2024-12-18T12:49:28+0100', 'type': 'content', 'section': 'politica', 'url': 'https://www.corriere.it/politica/24_dicembre_18/toti-il-giudice-da-l-ok-al-patteggiamento-2-anni-e-3-mesi-per-corruzione-li-scontera-alla-lilt-b41e0a3b-d99e-4622-964b-3380317b7xlk.shtml'}, page_content=\"\\nSi è chiusa la vicenda giudiziaria per Giovanni Toti. Oggi, sette mesi dopo i suoi arresti domiciliari, l'ex governatore della Regione Liguria ha patteggiato due anni e tre mesi, convertiti in 1620 ore di lavori socialmente utili. Il giudice per l'udienza preliminare Matteo Buffoni ha ratificato quanto concordato tra Toti e la procura. I lavori socialmente utili verranno svolti, da gennaio, presso la Lega italiana per la lotta ai tumori di Genova: si occuperà della comunicazione e risponderà al telefono per gestire le prenotazioni dei pazienti. Potrà fare anche più di 15 ore settimanali, avendo ricevuto una deroga, e non solo nel capoluogo ligure ma su tutto il territorio italiano.\\xa0\"),\n",
       " Document(metadata={'id': '4acf1077-8f43-4d60-a480-062c7b8bad98', 'relevance_score': 0.001716033, 'author': 'Claudio Bozza', 'date': '2024-12-18T11:46:55+0100', 'type': 'content', 'section': 'politica', 'url': 'https://www.corriere.it/politica/24_dicembre_18/valditara-fa-causa-a-lagioia-il-ministro-chiede-20-mila-euro-allo-scrittore-ecco-perche-4d6d16fa-cfc0-434b-a431-184bf2bddxlk.shtml'}, page_content=\"\\nTra politica e mondo della cultura, il rumore su questo nuovo scontro continua a essere forte: «Davvero Valditara vuole vivere in un Paese in cui chi osa dire che un ministro scrive male deve pagare 20mila euro? - è la riflessione di Lagioia in un'intervista a La Stampa - Sarebbe orribile». E poi: «Non chiedo la grazia a Meloni, certo sono deluso perché aveva aperto al dissenso Giuli si esprima sulla vicenda, altrimenti non può essere un autorevole interlocutore per l’editoria».\"),\n",
       " Document(metadata={'id': 'f76a3639-0694-40c7-9734-193a4d77487b', 'relevance_score': 0.0006084116, 'author': 'Redazione Politica', 'date': '2024-12-18T12:49:28+0100', 'type': 'content', 'section': 'politica', 'url': 'https://www.corriere.it/politica/24_dicembre_18/toti-il-giudice-da-l-ok-al-patteggiamento-2-anni-e-3-mesi-per-corruzione-li-scontera-alla-lilt-b41e0a3b-d99e-4622-964b-3380317b7xlk.shtml'}, page_content=\"\\n«Il patteggiamento è un'applicazione di pena che prescinde sia da un accertamento della responsabilità che da una ammissione della responsabilità». Lo ha detto il difensore di Giovanni Toti, l'avvocato Stefano Savi. Toti non potrà ricandidarsi nei prossimi due anni, «ma è un capitolo che non è nelle cose in questo momento», precisa Savi che sul ruolo di Toti nella Lilt dice: «Non sarà un ufficio stampa, si occuperà di campagne rivolte alla prevenzione».\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = opensearch_retriever_wrapper.retriever.invoke(question)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire chain\n",
    "Testiamo l'intera catena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "chain = get_chain(opensearch_retriever_wrapper.retriever, LLM_QUERY_CREATOR, rag_utils.user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gli articoli [1] e [3] della sezione politica parlano del patteggiamento giudiziario dell'ex governatore della Regione Liguria, Giovanni Toti. Oggi, sette mesi dopo i suoi arresti domiciliari, Toti ha patteggiato due anni e tre mesi, convertiti in 1620 ore di lavori socialmente utili. L'avvocato Stefano Savi ha precisato che il patteggiamento prescinde sia da un accertamento della responsabilità che da una ammissione della stessa. Toti non potrà ricandidarsi nei prossimi due anni, ma l'avvocato non ha fornito ulteriori dettagli su questo aspetto."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(question):\n",
    "    q = chunk.get('question',None)\n",
    "    if q is not None:\n",
    "        continue\n",
    "    c = chunk.get('context',None)\n",
    "    if c is not None:\n",
    "        #print(f\"Context: {c}\", end='', flush=True)\n",
    "        continue\n",
    "    print(chunk.get('answer',None), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
